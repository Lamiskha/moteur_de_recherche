{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd2b099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "SnowballStemmer.languages\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import string\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6a08a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrez votre mot : mm\n",
      "cos entre C:\\Users\\HP\\NLTK\\corps\\file2.txt et C:\\Users\\HP\\NLTK\\corps\\file1.txt  = 0.7227485729981566\n",
      "l'angle :  43.71812353745216\n",
      "cos entre C:\\Users\\HP\\NLTK\\corps\\file3.txt et C:\\Users\\HP\\NLTK\\corps\\file1.txt  = 0.5879065096096708\n",
      "l'angle :  53.99141190987979\n",
      "cos entre C:\\Users\\HP\\NLTK\\corps\\file4.txt et C:\\Users\\HP\\NLTK\\corps\\file1.txt  = 0.38087260847594373\n",
      "l'angle :  67.61225549050198\n",
      "cos entre C:\\Users\\HP\\NLTK\\corps\\file3.txt et C:\\Users\\HP\\NLTK\\corps\\file2.txt  = 0.6698277952604175\n",
      "l'angle :  47.94622464962414\n",
      "cos entre C:\\Users\\HP\\NLTK\\corps\\file4.txt et C:\\Users\\HP\\NLTK\\corps\\file2.txt  = 0.6694188517266486\n",
      "l'angle :  47.977772665836326\n",
      "cos entre C:\\Users\\HP\\NLTK\\corps\\file4.txt et C:\\Users\\HP\\NLTK\\corps\\file3.txt  = 0.4031254924501004\n",
      "l'angle :  66.22628592304834\n",
      "['juli', 'love', 'linda', 'love', 'jane', 'like', 'juli', 'love', 'harri', 'like', 'rone', 'juli', 'love', 'jane', 'like', 'jini', 'hate']\n",
      "\n",
      "17\n",
      "[\n",
      "  {\n",
      "    \"juli\": [\n",
      "      \"file1.txt\",\n",
      "      \"file2.txt\",\n",
      "      \"file3.txt\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"love\": [\n",
      "      \"file1.txt\",\n",
      "      \"file2.txt\",\n",
      "      \"file3.txt\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"linda\": [\n",
      "      \"file1.txt\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"love\": [\n",
      "      \"file1.txt\",\n",
      "      \"file2.txt\",\n",
      "      \"file3.txt\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"jane\": [\n",
      "      \"file2.txt\",\n",
      "      \"file4.txt\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"like\": [\n",
      "      \"file2.txt\",\n",
      "      \"file3.txt\",\n",
      "      \"file4.txt\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"juli\": [\n",
      "      \"file1.txt\",\n",
      "      \"file2.txt\",\n",
      "      \"file3.txt\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"love\": [\n",
      "      \"file1.txt\",\n",
      "      \"file2.txt\",\n",
      "      \"file3.txt\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"harri\": [\n",
      "      \"file3.txt\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"like\": [\n",
      "      \"file2.txt\",\n",
      "      \"file3.txt\",\n",
      "      \"file4.txt\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"rone\": [\n",
      "      \"file3.txt\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"juli\": [\n",
      "      \"file1.txt\",\n",
      "      \"file2.txt\",\n",
      "      \"file3.txt\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"love\": [\n",
      "      \"file1.txt\",\n",
      "      \"file2.txt\",\n",
      "      \"file3.txt\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"jane\": [\n",
      "      \"file2.txt\",\n",
      "      \"file4.txt\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"like\": [\n",
      "      \"file2.txt\",\n",
      "      \"file3.txt\",\n",
      "      \"file4.txt\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"jini\": [\n",
      "      \"file4.txt\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"hate\": [\n",
      "      \"file4.txt\"\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "folderpath = r\"C:\\Users\\HP\\NLTK\\corps\"\n",
    "filepath  = [os.path.join(folderpath, name) for name in os.scandir(folderpath)]\n",
    "all_files = []\n",
    "lis = []\n",
    "ext = ('.txt')\n",
    "stps = stopwords.words('english')\n",
    "sn_stemmer = SnowballStemmer('english')\n",
    "array = [\"a\",\"to\", \"the\",\"there\",\"etc\", \"that\", \"this\", \"about\", \"it\", \"above\", \"after\", \"again\", \"against\", \"all\",\n",
    "         \"am\", \"an\", \"and\", \"any\", \"are\", \"aren't\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\",\n",
    "         \"below\", \"between\", \"both\", \"but\", \"by\", \"can't\", \"cannot\", \"could\", \"couldn't\",\n",
    "         \"did\", \"didn't\", \"do\", \"does\", \"doesn't\", \"doing\", \"don't\", \"down\",\n",
    "         \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"hadn't\",\n",
    "         \"has\", \"hasn't\", \"have\", \"haven't\", \"having\", \"he\", \"he'd\", \"he'll\",\n",
    "         \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\",\n",
    "         \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\",\n",
    "         \"is\", \"isn't\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\",\n",
    "         \"mustn't\", \"my\", \"myself\", \"no\", \"nor\", \"not\", \"of\", \"off\", \"on\", \"once\",\n",
    "         \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"the\"]\n",
    "stp = [\"“\", \"(\", \")\", \"=\", \",\", \":\", \"/\", \".\", \"”\"]\n",
    "rqt = input(\"Entrez votre mot : \")    \n",
    "for path in filepath:\n",
    "    current_index = filepath.index(path)\n",
    "   # print(current_index)\n",
    "    with open(path,'r', encoding=\"utf-8\" ) as f:\n",
    "        if path.endswith(ext):\n",
    "            file = f.read()\n",
    "            file = file.translate(str.maketrans('', '', string.punctuation))\n",
    "          \n",
    "    for i in range(current_index , len(filepath)):\n",
    "        if (i == len(filepath)-1):\n",
    "            break\n",
    "       \n",
    "        f2 = open(filepath[i+1],\"r\",encoding=\"utf-8\")\n",
    "        z = f2.read()\n",
    "        z = z.translate(str.maketrans('', '', string.punctuation))\n",
    "        #print(z)\n",
    "        c = [file , z]\n",
    "        tfidf_v = TfidfVectorizer()\n",
    "        tfidf_m = tfidf_v.fit_transform(c)\n",
    "        cos_sim = cosine_similarity(tfidf_m)\n",
    "        print(f'cos entre {filepath[i+1]} et {path}  = {cos_sim[0,1]}')\n",
    "        angle = math.acos(cos_sim[0,1])\n",
    "        print(\"l'angle : \", math.degrees(angle))\n",
    "        #with open(path,'r', encoding=\"utf-8\" ) as f:\n",
    "          #  if path.endswith(ext):\n",
    "            #    file = f.read()\n",
    "             #   file = file.translate(str.maketrans('', '', string.punctuation)) \n",
    "              #  print(file.split())\n",
    "                \n",
    "                \n",
    "for path in filepath:\n",
    "    with open(path,'r', encoding=\"utf-8\" ) as f:\n",
    "         if path.endswith(ext):\n",
    "            file = f.read()\n",
    "            file = file.translate(str.maketrans('', '', string.punctuation))\n",
    "            for k in stp:\n",
    "                file = file.replace(k, \"\")\n",
    "                #all_files.append(file.split())      \n",
    "            for i in file.split():\n",
    "                 if(i.lower() not in stps):\n",
    "                    k = sn_stemmer.stem(i)\n",
    "                    all_files.append(k)\n",
    "                  #  data ={ path.split('\\\\')[-1] :{k:file.split().count(i) }}       \n",
    "                   # lis.append(data)\n",
    "                    if(sn_stemmer.stem(rqt) == k):\n",
    "                        print(f'{sn_stemmer.stem(rqt)}==>{i}')\n",
    "                        print(\"le mot exist dans: \",path.split('\\\\')[-1])\n",
    "\n",
    "\n",
    "                    \n",
    "t = []\n",
    "for j in all_files :\n",
    "    t =[]\n",
    "    for path in filepath:\n",
    "        with open(path,'r', encoding=\"utf-8\" ) as f:\n",
    "             if path.endswith(ext):\n",
    "                file = f.read()\n",
    "                file = file.translate(str.maketrans('', '', string.punctuation))\n",
    "                for x in file.split():\n",
    "                    if (sn_stemmer.stem(x) == j):\n",
    "                        t.append(path.split('\\\\')[-1])\n",
    "                        \n",
    "    data ={j: list(dict.fromkeys(t))} \n",
    "    lis.append(data) \n",
    "    \n",
    "     \n",
    "print(f'{all_files}\\n')\n",
    "print(len(all_files))\n",
    "print(json.dumps(lis,indent =2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d814c22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
